{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various experiments\n",
    "\n",
    "**NOTE** all of these experiments were run on the faulty Hungarian data (unless explicitly stated otherwise).\n",
    "\n",
    "## Data modification\n",
    "\n",
    "### POS moved\n",
    "\n",
    "POS moved to the end of the lemma.\n",
    "\n",
    "### Bigram\n",
    "\n",
    "~~~\n",
    "mé éz zé éd de es       mé éz zé éd de es se ek ki ig   N;TERM;PL\n",
    "le ep pé én ny yh ha al le ep pé én ny yh ha al ln na ak        N;DAT;SG\n",
    "ag gy yo on nv vá ág    ag gy yo on nv vá ág gn né ék   V;COND;PRS;INDF;1;SG\n",
    "~~~\n",
    "\n",
    "### Data augmentation - symmetric pairs\n",
    "\n",
    "Generate every possible pair of inflections and lemma of the same word form.\n",
    "\n",
    "~~~\n",
    "<W> a b i o g é n b e </W> <S> N IN+ALL SG </S> <T> LEMMA </T>  a b i o g é n\n",
    "<W> a b i o g é n b e </W> <S> N IN+ALL SG </S> <T> N IN+ALL SG </T>    a b i o g é n b e\n",
    "<W> a b i o g é n </W> <S> LEMMA </S> <T> LEMMA </T>    a b i o g é n\n",
    "<W> a b i o g é n </W> <S> LEMMA </S> <T> N IN+ALL SG </T>      a b i o g é n b e\n",
    "~~~\n",
    "\n",
    "### Reverse target sequence\n",
    "\n",
    "~~~\n",
    "borotva zohkávtorob     N;AT+ALL;PL\n",
    "kigúnyol        avloynúgik      V.CVB\n",
    "földcsuszamlás  lóbsálmazsuscdlöf       N;ON+ABL;SG\n",
    "hírlap  kanpalríh       N;DAT;SG\n",
    "~~~\n",
    "\n",
    "### Mix with other languages\n",
    "\n",
    "Merge and shuffle data in two or more languages.\n",
    "\n",
    "I tried Finnish and Welsh.\n",
    "\n",
    "### Filtering incorrect Hungarian examples\n",
    "\n",
    "About 10% of the Hungarian train and dev data are incorrect due to Wiktionary parse errors. I filtered these and trained some of the models on the smaller correct dataset.\n",
    "\n",
    "## Models\n",
    "\n",
    "### Luong attention\n",
    "\n",
    "Vanilla seq2seq + Luong attention.\n",
    "\n",
    "Differences from the 2016 winner:\n",
    "\n",
    "* Luong attention instead of Bahdanau attention. Reason: haven't implemented the other yet.\n",
    "* LSTMs instead of GRUs. Reason: in all my other experiments LSTMs outperformed GRUs and I'm not sure why they use GRUs.\n",
    "\n",
    "The input data is converted to:\n",
    "\n",
    "~~~\n",
    "<S> a b i o g é n </S> <T> N IN+ALL SG </T>      a b i o g é n b e\n",
    "~~~\n",
    "\n",
    "### Two-headed attention\n",
    "\n",
    "The lemma and the tags are encoded separately and two attention separately attend to them while decoding.\n",
    "\n",
    "### Misc variations\n",
    "\n",
    "#### L1 regularization\n",
    "\n",
    "Ran a few experiments, not planning anything with it right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "\n",
    "def compute_sparsity(model_fn, threshold=10e-3):\n",
    "    is_zero = 0\n",
    "    non_zero = 0\n",
    "    for name, tensor in torch.load(model_fn).items():\n",
    "        m = tensor.cpu().numpy()\n",
    "        close = len(np.where(np.abs(m) <= threshold)[0])\n",
    "        is_zero += close\n",
    "        non_zero += (m.size - close)\n",
    "    return is_zero, non_zero, is_zero / (is_zero + non_zero)\n",
    "    \n",
    "    \n",
    "def get_min_loss(row):\n",
    "    min_idx, min_dev_loss = min(enumerate(row['dev_loss']), key=lambda x: x[1])\n",
    "    min_train_loss = row['train_loss'][min_idx]\n",
    "    row['min_dev_loss'] = min_dev_loss\n",
    "    row['min_train_loss'] = min_train_loss\n",
    "    return row\n",
    "    \n",
    "    \n",
    "def extract_language_name(field):\n",
    "    fn = field.split('/')[-1]\n",
    "    if 'dev' in fn:\n",
    "        return '-'.join(fn.split('-')[:-1])\n",
    "    return '-'.join(fn.split('-')[:-2])\n",
    "    \n",
    "\n",
    "def load_res_dir(basedir, include_sparsity=False):\n",
    "    experiments = []\n",
    "    for subdir in os.scandir(basedir):\n",
    "        exp_d = {}\n",
    "        with open(os.path.join(subdir.path, \"config.yaml\")) as f:\n",
    "            exp_d.update(yaml.load(f))\n",
    "        res_fn = os.path.join(subdir.path, \"result.yaml\")\n",
    "        if os.path.exists(res_fn):\n",
    "            with open(os.path.join(subdir.path, \"result.yaml\")) as f:\n",
    "                exp_d.update(yaml.load(f))\n",
    "        else:\n",
    "            continue\n",
    "        dev_acc_path = os.path.join(subdir.path, \"dev.word_accuracy\")\n",
    "        if os.path.exists(dev_acc_path):\n",
    "            with open(dev_acc_path) as f:\n",
    "                exp_d['dev_acc'] = float(f.read())\n",
    "        else:\n",
    "            print(\"Dev accuracy file does not exist in dir: {}\".format(subdir.path))\n",
    "        if include_sparsity:\n",
    "            exp_d['sparsity'] = compute_sparsity(os.path.join(subdir.path, \"model\"), 10e-4)\n",
    "        experiments.append(exp_d)\n",
    "    experiments = pd.DataFrame(experiments)\n",
    "    if include_sparsity:\n",
    "        experiments['sparsity_ratio'] = experiments['sparsity'].apply(lambda x: x[2])\n",
    "    experiments['language'] = experiments.dev_file.apply(extract_language_name)\n",
    "    experiments = experiments.apply(get_min_loss, axis=1)\n",
    "    experiments = experiments[experiments['dev_acc'].notnull()]\n",
    "    experiments = experiments[experiments['dev_loss'].notnull()]\n",
    "    experiments['train_size'] = experiments['train_file'].apply(lambda fn: fn.split('-')[-1])\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data modification\n",
    "\n",
    "### POS moved\n",
    "\n",
    "POS moved to the end of the lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.854"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"../../exps/sigmorphon_2018/pos_moved/\"\n",
    "\n",
    "all_experiments = load_res_dir(exp_dir)\n",
    "all_experiments['exp_type'] = 'pos_moved'\n",
    "all_experiments.dev_acc.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram\n",
    "\n",
    "~~~\n",
    "mé éz zé éd de es       mé éz zé éd de es se ek ki ig   N;TERM;PL\n",
    "le ep pé én ny yh ha al le ep pé én ny yh ha al ln na ak        N;DAT;SG\n",
    "ag gy yo on nv vá ág    ag gy yo on nv vá ág gn né ék   V;COND;PRS;INDF;1;SG\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.569"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"../../exps/sigmorphon_2018/bigram/\"\n",
    "\n",
    "experiments = load_res_dir(exp_dir)\n",
    "experiments['exp_type'] = 'bigram'\n",
    "all_experiments = pd.concat((all_experiments, experiments))\n",
    "experiments.dev_acc.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation - symmetric pairs\n",
    "\n",
    "Generate every possible pair of inflections and lemma of the same word form.\n",
    "\n",
    "~~~\n",
    "<W> a b i o g é n b e </W> <S> N IN+ALL SG </S> <T> LEMMA </T>  a b i o g é n\n",
    "<W> a b i o g é n b e </W> <S> N IN+ALL SG </S> <T> N IN+ALL SG </T>    a b i o g é n b e\n",
    "<W> a b i o g é n </W> <S> LEMMA </S> <T> LEMMA </T>    a b i o g é n\n",
    "<W> a b i o g é n </W> <S> LEMMA </S> <T> N IN+ALL SG </T>      a b i o g é n b e\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max dev accuracy on the enhanced data: 0.97668\n"
     ]
    }
   ],
   "source": [
    "exp_dir = \"../../exps/sigmorphon_2018/hun_enhanced/\"\n",
    "\n",
    "experiments = load_res_dir(exp_dir)\n",
    "print(\"Max dev accuracy on the enhanced data: {}\".format(experiments.dev_acc.max()))\n",
    "\n",
    "for row in experiments.iterrows():\n",
    "    with open(os.path.join(row[1].experiment_dir, 'real_dev.word_accuracy')) as f:\n",
    "        experiments.loc[row[0], 'dev_acc'] = float(f.read())\n",
    "        \n",
    "experiments['exp_type'] = 'symmetric_augmented'\n",
    "all_experiments = pd.concat((all_experiments, experiments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse target sequence\n",
    "\n",
    "~~~\n",
    "borotva zohkávtorob     N;AT+ALL;PL\n",
    "kigúnyol        avloynúgik      V.CVB\n",
    "földcsuszamlás  lóbsálmazsuscdlöf       N;ON+ABL;SG\n",
    "hírlap  kanpalríh       N;DAT;SG\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.859"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"../../exps/sigmorphon_2018/hun_rev/\"\n",
    "\n",
    "experiments = load_res_dir(exp_dir)\n",
    "experiments['exp_type'] = 'reverse_target'\n",
    "all_experiments = pd.concat((all_experiments, experiments))\n",
    "experiments.dev_acc.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mix with other languages\n",
    "\n",
    "Merge and shuffle data in two or more languages.\n",
    "\n",
    "#### Hungarian and Finnish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.792)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"../../exps/hun_fin/\"\n",
    "\n",
    "hun_fin = load_res_dir(exp_dir)\n",
    "len(hun_fin), hun_fin.dev_acc.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hungarian and Welsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.812727)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"../../exps/hun_welsh/\"\n",
    "\n",
    "hun_welsh = load_res_dir(exp_dir)\n",
    "len(hun_welsh), hun_welsh.dev_acc.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering incorrect Hungarian examples\n",
    "\n",
    "About 10% of the Hungarian train and dev data are incorrect due to Wiktionary parse errors. I filtered these and trained some of the models on the smaller correct dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### Luong attention\n",
    "\n",
    "Vanilla seq2seq + Luong attention.\n",
    "\n",
    "Differences from the 2016 winner:\n",
    "\n",
    "* Luong attention instead of Bahdanau attention. Reason: haven't implemented the other yet.\n",
    "* LSTMs instead of GRUs. Reason: in all my other experiments LSTMs outperformed GRUs and I'm not sure why they use GRUs.\n",
    "\n",
    "The input data is converted to:\n",
    "\n",
    "~~~\n",
    "<S> a b i o g é n </S> <T> N IN+ALL SG </T>      a b i o g é n b e\n",
    "~~~\n",
    "\n",
    "### Two-headed attention\n",
    "\n",
    "The lemma and the tags are encoded separately and two attention separately attend to them while decoding.\n",
    "\n",
    "### Hard monotonic attention\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
