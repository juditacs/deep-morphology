{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various experiments\n",
    "\n",
    "**NOTE** all of these experiments were run on the faulty Hungarian data (unless explicitly stated otherwise).\n",
    "\n",
    "## Data modification\n",
    "\n",
    "### POS moved\n",
    "\n",
    "POS moved to the end of the lemma.\n",
    "\n",
    "### Bigram\n",
    "\n",
    "~~~\n",
    "mé éz zé éd de es       mé éz zé éd de es se ek ki ig   N;TERM;PL\n",
    "le ep pé én ny yh ha al le ep pé én ny yh ha al ln na ak        N;DAT;SG\n",
    "ag gy yo on nv vá ág    ag gy yo on nv vá ág gn né ék   V;COND;PRS;INDF;1;SG\n",
    "~~~\n",
    "\n",
    "### Data augmentation - symmetric pairs\n",
    "\n",
    "Generate every possible pair of inflections and lemma of the same word form.\n",
    "\n",
    "~~~\n",
    "<W> a b i o g é n b e </W> <S> N IN+ALL SG </S> <T> LEMMA </T>  a b i o g é n\n",
    "<W> a b i o g é n b e </W> <S> N IN+ALL SG </S> <T> N IN+ALL SG </T>    a b i o g é n b e\n",
    "<W> a b i o g é n </W> <S> LEMMA </S> <T> LEMMA </T>    a b i o g é n\n",
    "<W> a b i o g é n </W> <S> LEMMA </S> <T> N IN+ALL SG </T>      a b i o g é n b e\n",
    "~~~\n",
    "\n",
    "### Reverse target sequence\n",
    "\n",
    "~~~\n",
    "borotva zohkávtorob     N;AT+ALL;PL\n",
    "kigúnyol        avloynúgik      V.CVB\n",
    "földcsuszamlás  lóbsálmazsuscdlöf       N;ON+ABL;SG\n",
    "hírlap  kanpalríh       N;DAT;SG\n",
    "~~~\n",
    "\n",
    "### Mix with other languages\n",
    "\n",
    "Merge and shuffle data in two or more languages.\n",
    "\n",
    "I tried Finnish and Welsh.\n",
    "\n",
    "### Filtering incorrect Hungarian examples\n",
    "\n",
    "About 10% of the Hungarian train and dev data are incorrect due to Wiktionary parse errors. I filtered these and trained some of the models on the smaller correct dataset.\n",
    "\n",
    "## Models\n",
    "\n",
    "### Luong attention\n",
    "\n",
    "Vanilla seq2seq + Luong attention.\n",
    "\n",
    "Differences from the 2016 winner:\n",
    "\n",
    "* Luong attention instead of Bahdanau attention. Reason: haven't implemented the other yet.\n",
    "* LSTMs instead of GRUs. Reason: in all my other experiments LSTMs outperformed GRUs and I'm not sure why they use GRUs.\n",
    "\n",
    "The input data is converted to:\n",
    "\n",
    "~~~\n",
    "<S> a b i o g é n </S> <T> N IN+ALL SG </T>      a b i o g é n b e\n",
    "~~~\n",
    "\n",
    "### Two-headed attention\n",
    "\n",
    "The lemma and the tags are encoded separately and two attention separately attend to them while decoding.\n",
    "\n",
    "### Misc variations\n",
    "\n",
    "#### L1 regularization\n",
    "\n",
    "Ran a few experiments, not planning anything with it right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import yaml\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sparsity(model_fn, threshold=10e-3):\n",
    "    is_zero = 0\n",
    "    non_zero = 0\n",
    "    for name, tensor in torch.load(model_fn).items():\n",
    "        m = tensor.cpu().numpy()\n",
    "        close = len(np.where(np.abs(m) <= threshold)[0])\n",
    "        is_zero += close\n",
    "        non_zero += (m.size - close)\n",
    "    return is_zero, non_zero, is_zero / (is_zero + non_zero)\n",
    "    \n",
    "    \n",
    "def get_min_loss(row):\n",
    "    min_idx, min_dev_loss = min(enumerate(row['dev_loss']), key=lambda x: x[1])\n",
    "    min_train_loss = row['train_loss'][min_idx]\n",
    "    row['min_dev_loss'] = min_dev_loss\n",
    "    row['min_train_loss'] = min_train_loss\n",
    "    return row\n",
    "    \n",
    "    \n",
    "def extract_language_name(field):\n",
    "    if \"hun\" in field:\n",
    "        return \"hungarian\"\n",
    "    fn = field.split('/')[-1]\n",
    "    if 'dev' in fn:\n",
    "        return '-'.join(fn.split('-')[:-1])\n",
    "    return '-'.join(fn.split('-')[:-2])\n",
    "    \n",
    "\n",
    "def extract_train_file_size(field):\n",
    "    if 'train' in field:\n",
    "        return field.split('-')[-1]\n",
    "    return 'high'\n",
    "\n",
    "\n",
    "def load_res_dir(basedir, include_sparsity=False):\n",
    "    experiments = []\n",
    "    for subdir in os.scandir(basedir):\n",
    "        exp_d = {}\n",
    "        with open(os.path.join(subdir.path, \"config.yaml\")) as f:\n",
    "            exp_d.update(yaml.load(f))\n",
    "        res_fn = os.path.join(subdir.path, \"result.yaml\")\n",
    "        if os.path.exists(res_fn):\n",
    "            with open(os.path.join(subdir.path, \"result.yaml\")) as f:\n",
    "                exp_d.update(yaml.load(f))\n",
    "        else:\n",
    "            continue\n",
    "        dev_acc_path = os.path.join(subdir.path, \"dev.word_accuracy\")\n",
    "        if os.path.exists(dev_acc_path):\n",
    "            with open(dev_acc_path) as f:\n",
    "                exp_d['dev_acc'] = float(f.read())\n",
    "        else:\n",
    "            print(\"Dev accuracy file does not exist in dir: {}\".format(subdir.path))\n",
    "        train_acc_path = os.path.join(subdir.path, \"train.word_accuracy\")\n",
    "        if os.path.exists(train_acc_path):\n",
    "            with open(train_acc_path) as f:\n",
    "                exp_d['train_acc'] = float(f.read())\n",
    "        else:\n",
    "            print(\"Train accuracy file does not exist in dir: {}\".format(subdir.path))\n",
    "        if include_sparsity:\n",
    "            exp_d['sparsity'] = compute_sparsity(os.path.join(subdir.path, \"model\"), 10e-4)\n",
    "        experiments.append(exp_d)\n",
    "    experiments = pd.DataFrame(experiments)\n",
    "    if include_sparsity:\n",
    "        experiments['sparsity_ratio'] = experiments['sparsity'].apply(lambda x: x[2])\n",
    "    experiments['language'] = experiments.dev_file.apply(extract_language_name)\n",
    "    experiments = experiments.apply(get_min_loss, axis=1)\n",
    "    experiments = experiments[experiments['dev_acc'].notnull()]\n",
    "    experiments = experiments[experiments['dev_loss'].notnull()]\n",
    "    experiments['train_size'] = experiments['train_file'].apply(extract_train_file_size)\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data modification\n",
    "\n",
    "### POS moved\n",
    "\n",
    "POS moved to the end of the lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.854"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"../../exps/sigmorphon_2018/pos_moved/\"\n",
    "\n",
    "all_experiments = load_res_dir(exp_dir)\n",
    "all_experiments['exp_type'] = 'pos_moved'\n",
    "all_experiments['data_corrected'] = False\n",
    "all_experiments.dev_acc.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram\n",
    "\n",
    "~~~\n",
    "mé éz zé éd de es       mé éz zé éd de es se ek ki ig   N;TERM;PL\n",
    "le ep pé én ny yh ha al le ep pé én ny yh ha al ln na ak        N;DAT;SG\n",
    "ag gy yo on nv vá ág    ag gy yo on nv vá ág gn né ék   V;COND;PRS;INDF;1;SG\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.569"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"../../exps/sigmorphon_2018/bigram/\"\n",
    "\n",
    "experiments = load_res_dir(exp_dir)\n",
    "experiments['exp_type'] = 'bigram'\n",
    "experiments['data_corrected'] = False\n",
    "all_experiments = pd.concat((all_experiments, experiments))\n",
    "experiments.dev_acc.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation - symmetric pairs\n",
    "\n",
    "This is done on **corrected pairs**.\n",
    "\n",
    "Generate every possible pair of inflections and lemma of the same word form.\n",
    "\n",
    "~~~\n",
    "<W> a b i o g é n b e </W> <S> N IN+ALL SG </S> <T> LEMMA </T>  a b i o g é n\n",
    "<W> a b i o g é n b e </W> <S> N IN+ALL SG </S> <T> N IN+ALL SG </T>    a b i o g é n b e\n",
    "<W> a b i o g é n </W> <S> LEMMA </S> <T> LEMMA </T>    a b i o g é n\n",
    "<W> a b i o g é n </W> <S> LEMMA </S> <T> N IN+ALL SG </T>      a b i o g é n b e\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max dev accuracy on the enhanced data: 0.97668\n"
     ]
    }
   ],
   "source": [
    "exp_dir = \"../../exps/sigmorphon_2018/hun_enhanced/\"\n",
    "\n",
    "experiments = load_res_dir(exp_dir)\n",
    "print(\"Max dev accuracy on the enhanced data: {}\".format(experiments.dev_acc.max()))\n",
    "\n",
    "for row in experiments.iterrows():\n",
    "    with open(os.path.join(row[1].experiment_dir, 'real_dev.word_accuracy')) as f:\n",
    "        experiments.loc[row[0], 'dev_acc'] = float(f.read())\n",
    "        \n",
    "experiments['exp_type'] = 'symmetric_augmented'\n",
    "experiments['data_corrected'] = True\n",
    "all_experiments = pd.concat((all_experiments, experiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>dev_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_type</th>\n",
       "      <th>data_corrected</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bigram</th>\n",
       "      <th>False</th>\n",
       "      <td>0.569000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_moved</th>\n",
       "      <th>False</th>\n",
       "      <td>0.854000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetric_augmented</th>\n",
       "      <th>True</th>\n",
       "      <td>0.940716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     dev_acc\n",
       "exp_type            data_corrected          \n",
       "bigram              False           0.569000\n",
       "pos_moved           False           0.854000\n",
       "symmetric_augmented True            0.940716"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_experiments.groupby(['exp_type', 'data_corrected']).dev_acc.max().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse target sequence\n",
    "\n",
    "~~~\n",
    "borotva zohkávtorob     N;AT+ALL;PL\n",
    "kigúnyol        avloynúgik      V.CVB\n",
    "földcsuszamlás  lóbsálmazsuscdlöf       N;ON+ABL;SG\n",
    "hírlap  kanpalríh       N;DAT;SG\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.859"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"../../exps/sigmorphon_2018/hun_rev/\"\n",
    "\n",
    "experiments = load_res_dir(exp_dir)\n",
    "experiments['exp_type'] = 'reverse_target'\n",
    "experiments['data_corrected'] = False\n",
    "all_experiments = pd.concat((all_experiments, experiments))\n",
    "experiments.dev_acc.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mix with other languages\n",
    "\n",
    "Merge and shuffle data in two or more languages.\n",
    "\n",
    "#### Hungarian and Finnish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.792)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"../../exps/hun_fin/\"\n",
    "\n",
    "hun_fin = load_res_dir(exp_dir)\n",
    "hun_fin[\"language\"] = \"hungarian+finnish\"\n",
    "len(hun_fin), hun_fin.dev_acc.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hungarian and Welsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.812727)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"../../exps/hun_welsh/\"\n",
    "\n",
    "hun_welsh = load_res_dir(exp_dir)\n",
    "hun_welsh[\"language\"] = \"hungarian+welsh\"\n",
    "len(hun_welsh), hun_welsh.dev_acc.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments = pd.concat((all_experiments, hun_fin, hun_welsh)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering incorrect Hungarian examples\n",
    "\n",
    "About 10% of the Hungarian train and dev data are incorrect due to Wiktionary parse errors. I filtered these and trained some of the models on the smaller correct dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = \"../../exps/sigmorphon_2018/hun_correct/\"\n",
    "exps = load_res_dir(exp_dir)\n",
    "\n",
    "exp_dir = \"../../exps/sigmorphon_2018/hun_correct_luong/\"\n",
    "exps = pd.concat((exps, load_res_dir(exp_dir)))\n",
    "\n",
    "exps['exp_type'] = 'basic'\n",
    "exps['data_corrected'] = True\n",
    "\n",
    "all_experiments = pd.concat((all_experiments, exps))\n",
    "all_experiments = all_experiments.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hungarian            222\n",
       "hungarian+finnish      5\n",
       "hungarian+welsh        5\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_experiments.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General experiments with two basic models\n",
    "\n",
    "\n",
    "### `LuongAttentionSeq2seq`\n",
    "\n",
    "Vanilla seq2seq + Luong attention.\n",
    "\n",
    "Differences from the 2016 winner:\n",
    "\n",
    "* Luong attention instead of Bahdanau attention. Reason: haven't implemented the other yet.\n",
    "* LSTMs instead of GRUs. Reason: in all my other experiments LSTMs outperformed GRUs and I'm not sure why they use GRUs.\n",
    "\n",
    "The input data is converted to:\n",
    "\n",
    "~~~\n",
    "<S> a b i o g é n </S> <T> N IN+ALL SG </T>      a b i o g é n b e\n",
    "~~~\n",
    "\n",
    "### `ReinflectionSeq2seq`: Two-headed attention\n",
    "\n",
    "The lemma and the tags are encoded separately and two attention separately attend to them while decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of experiments-per-experiment type\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>exp_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">LuongAttentionSeq2seq</th>\n",
       "      <th>basic</th>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_moved</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reverse_target</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetric_augmented</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ReinflectionSeq2seq</th>\n",
       "      <th>basic</th>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_moved</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             0\n",
       "model                 exp_type                \n",
       "LuongAttentionSeq2seq basic                418\n",
       "                      pos_moved             30\n",
       "                      reverse_target        55\n",
       "                      symmetric_augmented   35\n",
       "ReinflectionSeq2seq   basic                822\n",
       "                      bigram                 5\n",
       "                      pos_moved             31"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"../../exps/sigmorphon_2018/luong_hyperparam_search/\"\n",
    "\n",
    "exps = load_res_dir(exp_dir)\n",
    "\n",
    "exp_dir = \"../../exps/reinflection/\"\n",
    "exps = pd.concat((exps, load_res_dir(exp_dir)))\n",
    "exp_dir = \"../../exps/reinflection_ron/\"\n",
    "exps = pd.concat((exps, load_res_dir(exp_dir)))\n",
    "\n",
    "exps['data_corrected'] = False\n",
    "exps['exp_type'] = 'basic'\n",
    "all_experiments = pd.concat((all_experiments, exps))\n",
    "\n",
    "all_experiments = all_experiments.reset_index(drop=True)\n",
    "\n",
    "print(\"Number of experiments-per-experiment type\")\n",
    "all_experiments.groupby(['model', 'exp_type']).size().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highest and average Hungarian dev accuracy by experiment and data type\n",
    "\n",
    "(size is the number of entries in that group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_type</th>\n",
       "      <th>data_corrected</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">basic</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>LuongAttentionSeq2seq</th>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.697031</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReinflectionSeq2seq</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.559888</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th>LuongAttentionSeq2seq</th>\n",
       "      <td>0.939597</td>\n",
       "      <td>0.668775</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReinflectionSeq2seq</th>\n",
       "      <td>0.921700</td>\n",
       "      <td>0.581879</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram</th>\n",
       "      <th>False</th>\n",
       "      <th>ReinflectionSeq2seq</th>\n",
       "      <td>0.569000</td>\n",
       "      <td>0.231800</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">pos_moved</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>LuongAttentionSeq2seq</th>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.670133</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReinflectionSeq2seq</th>\n",
       "      <td>0.854000</td>\n",
       "      <td>0.632581</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reverse_target</th>\n",
       "      <th>False</th>\n",
       "      <th>LuongAttentionSeq2seq</th>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.783309</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetric_augmented</th>\n",
       "      <th>True</th>\n",
       "      <th>LuongAttentionSeq2seq</th>\n",
       "      <td>0.940716</td>\n",
       "      <td>0.807734</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               max      mean  \\\n",
       "exp_type            data_corrected model                                       \n",
       "basic               False          LuongAttentionSeq2seq  0.856000  0.697031   \n",
       "                                   ReinflectionSeq2seq    0.850000  0.559888   \n",
       "                    True           LuongAttentionSeq2seq  0.939597  0.668775   \n",
       "                                   ReinflectionSeq2seq    0.921700  0.581879   \n",
       "bigram              False          ReinflectionSeq2seq    0.569000  0.231800   \n",
       "pos_moved           False          LuongAttentionSeq2seq  0.846000  0.670133   \n",
       "                                   ReinflectionSeq2seq    0.854000  0.632581   \n",
       "reverse_target      False          LuongAttentionSeq2seq  0.859000  0.783309   \n",
       "symmetric_augmented True           LuongAttentionSeq2seq  0.940716  0.807734   \n",
       "\n",
       "                                                          size  \n",
       "exp_type            data_corrected model                        \n",
       "basic               False          LuongAttentionSeq2seq   327  \n",
       "                                   ReinflectionSeq2seq     412  \n",
       "                    True           LuongAttentionSeq2seq    61  \n",
       "                                   ReinflectionSeq2seq       5  \n",
       "bigram              False          ReinflectionSeq2seq       5  \n",
       "pos_moved           False          LuongAttentionSeq2seq    30  \n",
       "                                   ReinflectionSeq2seq      31  \n",
       "reverse_target      False          LuongAttentionSeq2seq    55  \n",
       "symmetric_augmented True           LuongAttentionSeq2seq    35  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hun = all_experiments[all_experiments.language=='hungarian']\n",
    "hun.groupby(['exp_type', 'data_corrected', 'model']).dev_acc.agg(['max', 'mean', 'size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 best Hungarian experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>exp_type</th>\n",
       "      <th>data_corrected</th>\n",
       "      <th>dev_acc</th>\n",
       "      <th>train_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>LuongAttentionSeq2seq</td>\n",
       "      <td>symmetric_augmented</td>\n",
       "      <td>True</td>\n",
       "      <td>0.940716</td>\n",
       "      <td>0.997445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>LuongAttentionSeq2seq</td>\n",
       "      <td>basic</td>\n",
       "      <td>True</td>\n",
       "      <td>0.939597</td>\n",
       "      <td>0.981328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>LuongAttentionSeq2seq</td>\n",
       "      <td>symmetric_augmented</td>\n",
       "      <td>True</td>\n",
       "      <td>0.931767</td>\n",
       "      <td>0.995542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>LuongAttentionSeq2seq</td>\n",
       "      <td>basic</td>\n",
       "      <td>True</td>\n",
       "      <td>0.929530</td>\n",
       "      <td>0.994633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>LuongAttentionSeq2seq</td>\n",
       "      <td>basic</td>\n",
       "      <td>True</td>\n",
       "      <td>0.928412</td>\n",
       "      <td>0.984571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>LuongAttentionSeq2seq</td>\n",
       "      <td>symmetric_augmented</td>\n",
       "      <td>True</td>\n",
       "      <td>0.927293</td>\n",
       "      <td>0.987501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>LuongAttentionSeq2seq</td>\n",
       "      <td>symmetric_augmented</td>\n",
       "      <td>True</td>\n",
       "      <td>0.927293</td>\n",
       "      <td>0.990958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>LuongAttentionSeq2seq</td>\n",
       "      <td>basic</td>\n",
       "      <td>True</td>\n",
       "      <td>0.927293</td>\n",
       "      <td>0.963104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>LuongAttentionSeq2seq</td>\n",
       "      <td>basic</td>\n",
       "      <td>True</td>\n",
       "      <td>0.927293</td>\n",
       "      <td>0.963886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>LuongAttentionSeq2seq</td>\n",
       "      <td>symmetric_augmented</td>\n",
       "      <td>True</td>\n",
       "      <td>0.925056</td>\n",
       "      <td>0.981264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model             exp_type data_corrected   dev_acc  \\\n",
       "100  LuongAttentionSeq2seq  symmetric_augmented           True  0.940716   \n",
       "192  LuongAttentionSeq2seq                basic           True  0.939597   \n",
       "75   LuongAttentionSeq2seq  symmetric_augmented           True  0.931767   \n",
       "193  LuongAttentionSeq2seq                basic           True  0.929530   \n",
       "215  LuongAttentionSeq2seq                basic           True  0.928412   \n",
       "84   LuongAttentionSeq2seq  symmetric_augmented           True  0.927293   \n",
       "67   LuongAttentionSeq2seq  symmetric_augmented           True  0.927293   \n",
       "196  LuongAttentionSeq2seq                basic           True  0.927293   \n",
       "189  LuongAttentionSeq2seq                basic           True  0.927293   \n",
       "96   LuongAttentionSeq2seq  symmetric_augmented           True  0.925056   \n",
       "\n",
       "     train_acc  \n",
       "100   0.997445  \n",
       "192   0.981328  \n",
       "75    0.995542  \n",
       "193   0.994633  \n",
       "215   0.984571  \n",
       "84    0.987501  \n",
       "67    0.990958  \n",
       "196   0.963104  \n",
       "189   0.963886  \n",
       "96    0.981264  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hun.loc[hun.dev_acc.sort_values(ascending=False)[:10].index][['model', 'exp_type', 'data_corrected', 'dev_acc', 'train_acc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other languages\n",
    "\n",
    "I ran the two basic models on all languages and data sizes at least once.\n",
    "\n",
    "## 100% languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>dev_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>friulian</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>kabardian</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>occitan</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>pashto</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>swahili</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>uzbek</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language  dev_acc\n",
       "20   friulian      1.0\n",
       "35  kabardian      1.0\n",
       "60    occitan      1.0\n",
       "66     pashto      1.0\n",
       "78    swahili      1.0\n",
       "87      uzbek      1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = all_experiments.groupby('language').dev_acc.max().to_frame().reset_index()\n",
    "m[m.dev_acc == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 accuracy languages :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>train_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>greenlandic</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>ingrian</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>karelian</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>kashubian</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>kazakh</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>khakas</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>mapudungun</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>middle-high-german</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>murrinhpatha</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>norman</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>old-irish</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>scottish-gaelic</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>tibetan</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>turkmen</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                language train_size\n",
       "1020         greenlandic        low\n",
       "1037             ingrian        low\n",
       "1048            karelian        low\n",
       "1050           kashubian        low\n",
       "1052              kazakh        low\n",
       "1054              khakas        low\n",
       "1086          mapudungun        low\n",
       "1091  middle-high-german        low\n",
       "1093        murrinhpatha        low\n",
       "1101              norman        low\n",
       "1124           old-irish        low\n",
       "1147     scottish-gaelic        low\n",
       "1174             tibetan        low\n",
       "1179             turkmen        low"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest[highest.dev_acc==0][['language', 'train_size']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-headed attention\n",
    "\n",
    "The lemma and the tags are encoded separately and two attention separately attend to them while decoding.\n",
    "\n",
    "### Hard monotonic attention\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
