model: SopaSeq2seq
dataset_class: InflectionDataset

embedding_size: 40
hidden_size: 64
num_layers: 2
dropout: 0.3

epochs: 100
batch_size: 128
share_vocab: true

optimizer: Adam

attention_variant: general

experiment_dir: exps/sopa
save_min_epoch: 5 

cpu_only: true
bias_scale_param: 0.1

patterns:
    3: 20
    4: 18
    7: 3

# options: encoder_hidden, sopa_hidden, both
decoder_hidden: both

# options: encoder_outputs, sopa_hiddens, both
attention_on: both

use_lstm: true
concat_sopa_to_decoder_input: true
